{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YeM0ZBWUVtXR"
   },
   "source": [
    "# <font color='red'>SGD Algorithm to predict movie ratings</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M2vyJqSlmmjM"
   },
   "source": [
    "**There will be some functions that start with the word \"grader\" ex: grader_matrix(), grader_mean(), grader_dim() etc, you should not change those function definition.<br><br>Every Grader function has to return True.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AL6njTf8WBO0"
   },
   "source": [
    "<pre>\n",
    "1. Download the data from <a href='https://drive.google.com/open?id=1-1z7iDB52cB6_JpO7Dqa-eOYSs-mivpq'> here </a>\n",
    "2. The data will be of this format, each data point is represented as a triplet of user_id, movie_id and rating \n",
    "<table>\n",
    "<tr><th>user_id</th><th>movie_id</th><th>rating</th></tr>\n",
    "<tr><td>77</td><td>236</td><td>3</td></tr>\n",
    "<tr><td>471</td><td>208</td><td>5</td></tr>\n",
    "<tr><td>641</td><td>401</td><td>4</td></tr>\n",
    "<tr><td>31</td><td>298</td><td>4</td></tr>\n",
    "<tr><td>58</td><td>504</td><td>5</td></tr>\n",
    "<tr><td>235</td><td>727</td><td>5</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89992"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading the required file\n",
    "movie_rating = pd.read_csv('ratings_train.csv')\n",
    "movie_rating.head()\n",
    "movie_rating.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "73dhFsT0WSSB"
   },
   "source": [
    "## <font color='red'>Task 1</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HY0frS6EWaEV"
   },
   "source": [
    "<font color='red'><b>Predict the rating for a given (user_id, movie_id) pair </b> </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R-ISYxaVbT8L"
   },
   "source": [
    "Predicted rating $\\hat{y}_{ij}$ for user i, movied j pair is calcuated as $\\hat{y}_{ij} = \\mu + b_i + c_j + u_i^T v_j$ , here we will be finding the best values of $b_{i}$ and $c_{j}$ using SGD algorithm with the optimization problem for N users and M movies is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Aj8SXeQWlZd"
   },
   "source": [
    "$$\n",
    "L = \\min_{ b, c, \\{ u_i \\}_{i=1}^N, \\{ v_j \\}_{j=1}^M}\n",
    "\\quad\n",
    "\\alpha \\Big(\n",
    "    \\sum_{j} \\sum_{k} v_{jk}^2 \n",
    "    + \\sum_{i} \\sum_{k} u_{ik}^2 \n",
    "    + \\sum_{i} b_i^2\n",
    "    + \\sum_{j} c_i^2\n",
    "    \\Big)\n",
    "+ \\sum_{i,j \\in \\mathcal{I}^{\\text{train}}}\n",
    "    (y_{ij} - \\mu - b_i - c_j - u_i^T v_j)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Q5bnWyZXrM7"
   },
   "source": [
    "<ul>\n",
    "<li><span class=\"math\">\\(\\mu\\)</span> : scalar mean rating</li>\n",
    "<li><span class=\"math\">\\(b_i\\)</span> : scalar bias term for user <span class=\"math\">\\(i\\)</span></li>\n",
    "<li><span class=\"math\">\\(c_j\\)</span> : scalar bias term for movie <span class=\"math\">\\(j\\)</span></li>\n",
    "<li><span class=\"math\">\\(u_i\\)</span> : K-dimensional vector for user <span class=\"math\">\\(i\\)</span></li>\n",
    "<li><span class=\"math\">\\(v_j\\)</span> : K-dimensional vector for movie <span class=\"math\">\\(j\\)</span></li>\n",
    "</ul>\n",
    "\n",
    " $ \\ $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q1cf4CunbEr4"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "*.  We will be giving you some functions, please write code in that functions only.\n",
    "\n",
    "*.  After every function, we will be giving you expected output, please make sure that you get that output. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Construct adjacency matrix with the given data, assuming its  <a href='https://en.wikipedia.org/wiki/Bipartite_graph'> weighted un-directed bi-partited graph</a> and the weight of each edge is the rating given by user to the movie\n",
    "\n",
    "<img src='https://i.imgur.com/rmUCGMb.jpg' width=200>\n",
    "\n",
    "   you can construct this matrix like $A[i][j]=r_{ij}$ here $i$ is user_id, $j$ is movie_id and $r_{ij}$ is rating given by user $i$ to the movie $j$\n",
    "\n",
    "   Hint : you can create adjacency matrix using <a href='https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html'> csr_matrix</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.linalg import svd\n",
    "# U, sigma, V_T = svd(adjacent_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U.shape, sigma.shape, V_T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZWQyB5hfy3u7"
   },
   "source": [
    "\n",
    "2. We will Apply SVD decomposition on the Adjaceny matrix <a href='https://stackoverflow.com/a/31528944/4084039'>link1</a>, <a href='https://machinelearningmastery.com/singular-value-decomposition-for-machine-learning/'> link2</a> and get three matrices $U, \\sum, V$ such that $U \\times \\sum \\times V^T = A$, <br> \n",
    "if $A$ is of dimensions $N \\times M$ then <br>\n",
    "U is of $N \\times k$, <br>\n",
    "$\\sum$ is of $k \\times k$ and <br>\n",
    "$V$ is $M \\times k$ dimensions. <br>\n",
    "\n",
    "   *.  So the matrix $U$ can be represented as matrix representation of users, where each row $u_{i}$ represents a k-dimensional vector for a user\n",
    "\n",
    "   *. So the matrix $V$ can be represented as matrix representation of movies, where each row $v_{j}$ represents a k-dimensional vector for a movie.\n",
    "3. Compute $\\mu$ , $\\mu$  represents the mean of all the rating given in the dataset.(write your code in <font color='blue'>def m_u()</font>)\n",
    "4. For each unique user initilize a bias value $B_{i}$ to zero, so if we have $N$ users $B$ will be a $N$ dimensional vector, the $i^{th}$ value of the $B$ will corresponds to the bias term for $i^{th}$ user (write your code in <font color='blue'>def initialize()</font>)\n",
    "\n",
    "5. For each unique movie initilize a bias value $C_{j}$ zero, so if we have $M$ movies $C$ will be a $M$ dimensional vector, the $j^{th}$ value of the $C$ will corresponds to the bias term for $j^{th}$ movie (write your code in <font color='blue'>def initialize()</font>)\n",
    "\n",
    "6. Compute dL/db_i (Write you code in <font color='blue'> def derivative_db()</font>)\n",
    "7. Compute dL/dc_j(write your code in <font color='blue'> def derivative_dc()</font>\n",
    "\n",
    "8. Print the mean squared error with predicted ratings.\n",
    "\n",
    "<pre>\n",
    "for each epoch:\n",
    "    for each pair of (user, movie):\n",
    "        b_i =  b_i - learning_rate * dL/db_i\n",
    "        c_j =  c_j - learning_rate * dL/dc_j\n",
    "predict the ratings with formula\n",
    "</pre>\n",
    "$\\hat{y}_{ij} = \\mu + b_i + c_j + \\text{dot_product}(u_i , v_j) $\n",
    "\n",
    "9. you can choose any learning rate and regularization term in the range $10^{-3}  \\text{ to } 10^2$  <br>\n",
    "  \n",
    "10. __bonus__: instead of using SVD decomposition you can learn the vectors $u_i$, $v_j$ with the help of SGD algo similar to $b_i$ and $c_j$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-aBnRepA6gy"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IP_6xMAZA4mE"
   },
   "source": [
    " # <font color='red'>Task 2 </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-9HCN_3WA2au"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovFCo1JCBIXM"
   },
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iVJo-3njBQLf"
   },
   "source": [
    "<font color='red'> Reading the csv file </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "hEhmfRD637EW",
    "outputId": "07189bd2-eb44-43c7-f225-022cc41d0ee6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>772</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>471</td>\n",
       "      <td>228</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>641</td>\n",
       "      <td>401</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>312</td>\n",
       "      <td>98</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58</td>\n",
       "      <td>504</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0      772       36       3\n",
       "1      471      228       5\n",
       "2      641      401       4\n",
       "3      312       98       4\n",
       "4       58      504       5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying sample contents\n",
    "import pandas as pd\n",
    "data=pd.read_csv('ratings_train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "66ibGJ74hCde",
    "outputId": "627b068c-baa2-4751-f4d5-03a7c8e8b77b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89992, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 943, 1662)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#doing the initilisation\n",
    "import pandas as pd\n",
    "user_info_data=pd.read_csv('ratings_train.csv')\n",
    "µ   = np.mean(user_info_data['rating'])\n",
    "b_i = np.ones(user_info_data.shape[0]) * 0.1\n",
    "c_i = np.ones(user_info_data.shape[0]) * 0.1\n",
    "u_i = user_info_data['user_id']\n",
    "v_j = user_info_data['item_id']\n",
    "\n",
    "ratings_details  = user_info_data['rating'].tolist()\n",
    "users_details    = u_i.tolist()\n",
    "movies_details   = v_j.tolist()\n",
    "\n",
    "len(np.unique(ratings_details)), len(np.unique(users_details)), len(np.unique(movies_details))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mvB8SDS_hW03"
   },
   "source": [
    "<font color='red'>Create your adjacency matrix </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t44MNT40hZQW"
   },
   "outputs": [],
   "source": [
    "#creating adjacent matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "adjacency_matrix = csr_matrix((ratings_details, (users_details,movies_details))).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-mCgC0WbhZTO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1681)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjacency_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4acJD4ujEtD6"
   },
   "source": [
    "<font color='cyan'>Grader function - 1</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2QuTzFBREsDV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_matrix(matrix):\n",
    "  assert(matrix.shape==(943,1681))\n",
    "  return True\n",
    "grader_matrix(adjacency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gXDf1RCUBsYN"
   },
   "source": [
    "<font color='red'> SVD decompostion</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OJPWI9VwD_ih"
   },
   "source": [
    "Sample code for SVD decompostion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "GATD35bmBszc",
    "outputId": "b3f57c71-7ce6-4796-ab6c-e7af570a864c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 5)\n",
      "(5,)\n",
      "(10, 5)\n"
     ]
    }
   ],
   "source": [
    "#computing svd with components 5\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "import numpy as np \n",
    "matrix = np.random.random((20, 10))\n",
    "U, Sigma, VT = randomized_svd(matrix, n_components=5,n_iter=5, random_state=None)\n",
    "print(U.shape)\n",
    "print(Sigma.shape)\n",
    "print(VT.T.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ePDgwALQEJoB"
   },
   "source": [
    "<font color='red'>Write your code for SVD decompostion</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZYnsKBmFEIg3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 943)\n",
      "(943,)\n",
      "(1681, 943)\n"
     ]
    }
   ],
   "source": [
    "# Please use adjacency_matrix as matrix for SVD decompostion\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "import numpy as np \n",
    "matrix = np.random.random((20, 10))\n",
    "U, Sigma, VT = randomized_svd(adjacency_matrix, n_components=943,n_iter=5, random_state=None)\n",
    "print(U.shape)\n",
    "print(Sigma.shape)\n",
    "print(VT.T.shape)\n",
    "# You can choose n_components as your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "83Vh4NoO_JyU"
   },
   "source": [
    "<font color='red'>Compute mean of ratings</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cBHuCn2QSEnl"
   },
   "outputs": [],
   "source": [
    "def m_u(ratings):\n",
    "    '''In this function, we will compute mean for all the ratings'''\n",
    "    # you can use mean() function to do this\n",
    "    # check this (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html) link for more details.\n",
    "    \n",
    "\n",
    "    return ratings.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iu1nn-1x3ebp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.529480398257623\n"
     ]
    }
   ],
   "source": [
    "mu=m_u(data['rating'])\n",
    "print(mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "76ooYQIdG_tf"
   },
   "source": [
    "<font color='cyan'>Grader function -2 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TZy1m67oG9r9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_mean(mu):\n",
    "  assert(np.round(mu,3)==3.529)\n",
    "  return True\n",
    "mu=m_u(data['rating'])\n",
    "grader_mean(mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qSvAW1X94g3G"
   },
   "source": [
    "<font color='red'>Initialize $B_{i}$ and $C_{j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qsOl-4xq5aUG"
   },
   "source": [
    "Hint : Number of rows of adjacent matrix corresponds to user dimensions($B_{i}$), number of columns of adjacent matrix corresponds to movie dimensions ($C_{j}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AyEJqPka4lBW"
   },
   "outputs": [],
   "source": [
    "def initialize(dim):\n",
    "    '''In this function, we will initialize bias value 'B' and 'C'.'''\n",
    "    # initalize the value to zeros \n",
    "    # return output as a list of zeros \n",
    "    return np.zeros(dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nlae9QAQ43Xz"
   },
   "outputs": [],
   "source": [
    "dim= U.shape[0] # give the number of dimensions for b_i (Here b_i corresponds to users)\n",
    "b_i=initialize(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwuopn4HoEbP"
   },
   "outputs": [],
   "source": [
    "dim= VT.shape[1] # give the number of dimensions for c_j (Here c_j corresponds to movies)\n",
    "c_j=initialize(dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nfPJ3_h6JIkI"
   },
   "source": [
    "<font color='cyan'>Grader function -3 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dQhiNjw0Hz4m"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_dim(b_i,c_j):\n",
    "  assert(len(b_i)==943 and np.sum(b_i)==0)\n",
    "  assert(len(c_j)==1681 and np.sum(c_j)==0)\n",
    "  return True\n",
    "grader_dim(b_i,c_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L = \\min_{ b, c, \\{ u_i \\}_{i=1}^N, \\{ v_j \\}_{j=1}^M}\n",
    "\\quad\n",
    "\\alpha \\Big(\n",
    "    \\sum_{j} \\sum_{k} v_{jk}^2 \n",
    "    + \\sum_{i} \\sum_{k} u_{ik}^2 \n",
    "    + \\sum_{i} b_i^2\n",
    "    + \\sum_{j} c_i^2\n",
    "    \\Big)\n",
    "+ \\sum_{i,j \\in \\mathcal{I}^{\\text{train}}}\n",
    "    (y_{ij} - \\mu - b_i - c_j - u_i^T v_j)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DTDK4ZR18MrZ"
   },
   "source": [
    "<font color='red'>Compute dL/db_i</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3NFzVC1N8S4L"
   },
   "outputs": [],
   "source": [
    "def derivative_db(user_id,item_id,rating,U1,V1,mu,alpha):\n",
    "    '''In this function, we will compute dL/db_i'''\n",
    "    first_term  =  2 * alpha * b_i[user_id]\n",
    "    second_term = -2 * (rating - mu - b_i[user_id] - c_j[item_id]- (np.dot(U1[user_id], V1.T[item_id])))\n",
    "    derivative_of_db_i = first_term + second_term\n",
    "    return derivative_of_db_i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ilISrTeQ0f0v"
   },
   "source": [
    "<font color='cyan'>Grader function -4 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wt5ixEVZ043U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9308283758773337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_db(value):\n",
    "    assert(np.round(value,3)==-0.931)\n",
    "    return True\n",
    "U1, Sigma, V1 = randomized_svd(adjacency_matrix, n_components=2,n_iter=5, random_state=24)\n",
    "# Please don't change random state\n",
    "# Here we are considering n_componets = 2 for our convinence\n",
    "alpha=0.01 \n",
    "value=derivative_db(312,98,4,U1,V1,mu,alpha)\n",
    "print(value)\n",
    "grader_db(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Kp0hC_b9v60"
   },
   "source": [
    "<font color='red'>Compute dL/dc_j</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAtSYMrc9UqJ"
   },
   "outputs": [],
   "source": [
    "def derivative_dc(user_id,item_id,rating,U1,V1,mu,alpha):\n",
    "    '''In this function, we will compute dL/dc_j'''\n",
    "    first_term  =  2 * alpha * c_j[user_id]\n",
    "    second_term = -2 * (rating - mu - b_i[user_id] - c_j[item_id]- (np.dot(U1[user_id], V1.T[item_id])))\n",
    "    derivative_of_dc_j =  first_term + second_term \n",
    "    return derivative_of_dc_j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lxkAm8aH1SBF"
   },
   "source": [
    "<font color='cyan'>Grader function - 5 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RaIN9yie1US8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.9290787114434913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_dc(value):\n",
    "    assert(np.round(value,3)==-2.929)\n",
    "    return True\n",
    "U1, Sigma, V1 = randomized_svd(adjacency_matrix, n_components=2,n_iter=5, random_state=24)\n",
    "# Please don't change random state\n",
    "# Here we are considering n_componets = 2 for our convinence\n",
    "r=0.01 \n",
    "value=derivative_dc(58,504,5,U1,V1,mu,alpha)\n",
    "print(value)\n",
    "grader_dc(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lg5XNbDWCIKI"
   },
   "source": [
    "<font color='red'>Compute MSE (mean squared error) for predicted ratings</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7WUjNy0TDQX6"
   },
   "source": [
    "for each epoch, print the MSE value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D2pCy1AKCafw"
   },
   "source": [
    "<pre>\n",
    "for each epoch:\n",
    "\n",
    "    for each pair of (user, movie):\n",
    "\n",
    "        b_i =  b_i - learning_rate * dL/db_i\n",
    "\n",
    "        c_j =  c_j - learning_rate * dL/dc_j\n",
    "\n",
    "predict the ratings with formula\n",
    "</pre>\n",
    "\n",
    "$\\hat{y}_{ij} = \\mu + b_i + c_j + \\text{dot_product}(u_i , v_j) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This was my inital try'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' This was my inital try'''\n",
    "# from tqdm import tqdm\n",
    "# import random\n",
    "\n",
    "# def predictions(users_details,movies_details,b_i, c_j):\n",
    "#     all_predictions = []\n",
    "#     for bi, cj, usrid, mvid in zip(b_i, c_j,users_details,movies_details):\n",
    "#         prediction = (mu - bi - cj - (np.dot(usrid, mvid)))\n",
    "#         all_predictions.append(prediction)\n",
    "#     return all_predictions\n",
    "# '''  intintializing the values '''\n",
    "# learning_rate = alpha = 0.01\n",
    "# ''' Looping through each epoch '''\n",
    "# for each_point in tqdm(range(0, 20)):\n",
    "#     ''' Iterating for each batch '''\n",
    "#     for usr_id, itm_id, ratngs in zip(u_i,v_j,ratings_details):\n",
    "#         ''' Getting random index '''\n",
    "#         b_i = b_i - learning_rate * derivative_db(usr_id,itm_id,ratngs,U,VT,mu,alpha)\n",
    "#         c_j = c_j - learning_rate * derivative_dc(usr_id,itm_id,ratngs,U,VT,mu,alpha)\n",
    "#     ''' stroing the optimized weigths and bais for each epoch'''\n",
    "#     model_preditions = predictions(u_i,v_j,b_i, c_j)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:17<00:00,  1.55s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# required details\n",
    "learning_rate= 0.001\n",
    "y=ratings_details\n",
    "all_predictions = []\n",
    "#running through 50 epochs\n",
    "for epoch in tqdm(range(50)):\n",
    "    #for all records of userdetails\n",
    "    for user_id,item_id,rating in zip(users_details,movies_details,ratings_details):\n",
    "        #calculating the derivates of b_i & c_j\n",
    "        b_i[user_id]= b_i[user_id] - learning_rate *derivative_db(user_id,item_id,rating,U,VT,mu,r)\n",
    "        c_j[item_id]= c_j[item_id] - learning_rate *derivative_dc(user_id,item_id,rating,U,VT,mu,r)\n",
    "    #calculating hte predicitons after each epoch\n",
    "    y_pred=[]\n",
    "    for user_id,item_id in zip(users_details,movies_details):\n",
    "        y_pred.append(mu+b_i[user_id]+c_j[item_id]+( np.dot(U[user_id].T, VT.T[item_id]) ) )\n",
    "    #calculating mse for each epoch \n",
    "    all_predictions.append(mean_squared_error(ratings_details,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plotting results\n",
    "plt.plot(all_predictions)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = []\n",
    "for user_id,item_id in zip(users_details,movies_details):\n",
    "        model_predictions.append(mu+b_i[user_id]+c_j[item_id]+(np.dot(U[user_id].T,VT.T[item_id])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.7611824800809774,\n",
       " 4.059265007772838,\n",
       " 3.884913542263527,\n",
       " 3.63837658315012,\n",
       " 4.235702221797333,\n",
       " 3.2313053811216133,\n",
       " 4.0963460897476285,\n",
       " 3.882127157897883,\n",
       " 3.887364309072743,\n",
       " 2.841358113657961,\n",
       " 3.0146833340519334,\n",
       " 4.140630734252557,\n",
       " 3.5062154752840877,\n",
       " 3.3597031437355644,\n",
       " 2.8093818371144974,\n",
       " 3.6688883924342455,\n",
       " 3.2857568867308906,\n",
       " 3.880827861813851,\n",
       " 3.8711143969360147,\n",
       " 2.736400113327132,\n",
       " 4.163455229179991,\n",
       " 4.346155615152665,\n",
       " 3.5446831223164565,\n",
       " 3.5773667055441467,\n",
       " 4.11342671568928,\n",
       " 4.4356967135003655,\n",
       " 3.997604073933184,\n",
       " 3.810257164397943,\n",
       " 3.8461975818423544,\n",
       " 3.3603395580648865,\n",
       " 4.100828933260428,\n",
       " 3.83488334074237,\n",
       " 3.8771297945664043,\n",
       " 3.143953078003329,\n",
       " 3.5551648648206284,\n",
       " 2.6049604503952737,\n",
       " 3.427107774050581,\n",
       " 3.9560971258336126,\n",
       " 4.10538488665526,\n",
       " 4.521356017148614,\n",
       " 4.269060014541589,\n",
       " 4.3400658530948295,\n",
       " 3.7072102860776264,\n",
       " 3.763135437213109,\n",
       " 2.7098609458403846,\n",
       " 3.119887006615999,\n",
       " 3.337668582419138,\n",
       " 3.3996808178286138,\n",
       " 3.762283005142332,\n",
       " 3.2408789093683774,\n",
       " 3.2534350707528845,\n",
       " 3.4097605431633573,\n",
       " 4.488210524682716,\n",
       " 3.2806103158762805,\n",
       " 3.2261767366542875,\n",
       " 3.30811440097104,\n",
       " 4.239691172996659,\n",
       " 3.369259475693882,\n",
       " 2.050182985227139,\n",
       " 4.750062220502152,\n",
       " 3.510843322930219,\n",
       " 4.274907348828458,\n",
       " 2.883815398012403,\n",
       " 2.862098159756,\n",
       " 2.6503655192762605,\n",
       " 3.4993878014862743,\n",
       " 4.519105281392697,\n",
       " 2.9410443069353196,\n",
       " 2.769928216842834,\n",
       " 3.747549429427516,\n",
       " 3.0191803862127906,\n",
       " 3.800128003916577,\n",
       " 3.047425827192908,\n",
       " 2.4433233626460877,\n",
       " 3.1224983265711126,\n",
       " 3.9692272481712028,\n",
       " 3.74199399062815,\n",
       " 2.4483123675586866,\n",
       " 2.4779992113113165,\n",
       " 3.662311768591242,\n",
       " 4.7284143030876775,\n",
       " 4.438742964730094,\n",
       " 3.3830483146618353,\n",
       " 2.994144858733208,\n",
       " 4.023490489456356,\n",
       " 4.307338652383497,\n",
       " 3.155285670574278,\n",
       " 3.0541813917417366,\n",
       " 4.495918474851492,\n",
       " 4.6380409107509495,\n",
       " 4.198149620089821,\n",
       " 3.3254456829378976,\n",
       " 4.2025416335326025,\n",
       " 2.438852912552976,\n",
       " 3.255059219346679,\n",
       " 2.225030201911625,\n",
       " 3.112623442887023,\n",
       " 2.998741285470495,\n",
       " 3.46644230823986,\n",
       " 2.425390674329011,\n",
       " 4.0395736553101305,\n",
       " 1.8490155993916346,\n",
       " 4.725146382920946,\n",
       " 3.472327614607294,\n",
       " 3.7359005040159623,\n",
       " 3.3386021812634032,\n",
       " 4.346225177966554,\n",
       " 3.5571832344636256,\n",
       " 4.207061935576009,\n",
       " 3.2534101692380135,\n",
       " 3.0485492030516066,\n",
       " 4.2006761037316975,\n",
       " 3.3568210969958825,\n",
       " 3.75265750946963,\n",
       " 3.842740801931438,\n",
       " 2.685463078766359,\n",
       " 4.454011642773938,\n",
       " 3.983358971675422,\n",
       " 3.4640170338384046,\n",
       " 4.217910126597042,\n",
       " 3.686633702801058,\n",
       " 2.914470242469389,\n",
       " 3.4105666746974586,\n",
       " 2.928919037993312,\n",
       " 4.107149588155945,\n",
       " 3.779797273629829,\n",
       " 3.6963820208500366,\n",
       " 3.7132477866001388,\n",
       " 3.852319733405734,\n",
       " 3.8633056841982043,\n",
       " 3.3530065081410694,\n",
       " 3.869337271787722,\n",
       " 3.5425477240578673,\n",
       " 2.9297534435233143,\n",
       " 3.5207837969143427,\n",
       " 4.53076246973319,\n",
       " 4.302267067020528,\n",
       " 3.6903888037084838,\n",
       " 4.017947325622155,\n",
       " 4.27521403478391,\n",
       " 3.193090529761517,\n",
       " 2.2952150435152054,\n",
       " 2.9468692020686236,\n",
       " 4.239597409985478,\n",
       " 4.1267882566808725,\n",
       " 1.1147667366418232,\n",
       " 3.2347310112464505,\n",
       " 3.653026401223972,\n",
       " 3.413050919858881,\n",
       " 3.007492635443649,\n",
       " 4.721707658241182,\n",
       " 3.1761643048104413,\n",
       " 4.2422039497257655,\n",
       " 3.8939095522201708,\n",
       " 4.169578257814025,\n",
       " 4.219563112574602,\n",
       " 4.648301817742475,\n",
       " 4.015633049915146,\n",
       " 3.980896377450937,\n",
       " 3.0203059665902563,\n",
       " 3.937885446862106,\n",
       " 3.912152293948589,\n",
       " 3.160468251102694,\n",
       " 4.3578644287430945,\n",
       " 3.8801779317251532,\n",
       " 2.773078215867366,\n",
       " 4.095868913576153,\n",
       " 4.007548914082393,\n",
       " 2.876864792850207,\n",
       " 2.957638414080825,\n",
       " 3.322732690005854,\n",
       " 4.207030494064512,\n",
       " 2.840094660577999,\n",
       " 3.596957653808644,\n",
       " 4.770647156241437,\n",
       " 3.0159753284488664,\n",
       " 3.057986576037952,\n",
       " 4.4427007751491825,\n",
       " 3.1032895493419703,\n",
       " 3.2509500779744784,\n",
       " 3.46887307503934,\n",
       " 3.05415672611932,\n",
       " 3.868743844510734,\n",
       " 3.6621196824086706,\n",
       " 3.4130963598351447,\n",
       " 3.7667840617032646,\n",
       " 2.8701360722077442,\n",
       " 2.9865351658277,\n",
       " 3.760117302170476,\n",
       " 3.0761581718513584,\n",
       " 4.383086711309606,\n",
       " 2.1915849479161094,\n",
       " 2.7028339204593093,\n",
       " 3.363721658873363,\n",
       " 2.5583388562159004,\n",
       " 4.081010928178577,\n",
       " 3.2392165639463486,\n",
       " 2.8873183498424577,\n",
       " 3.4831626350536116,\n",
       " 3.822712377095887,\n",
       " 3.8242227239378503,\n",
       " 3.986269879626191,\n",
       " 2.7717400367069027,\n",
       " 3.328089203361826,\n",
       " 3.4308646924069883,\n",
       " 1.6776984941195605,\n",
       " 3.597376318439813,\n",
       " 2.9301029962140026,\n",
       " 3.887613316691369,\n",
       " 3.545179643357139,\n",
       " 4.754404084111245,\n",
       " 4.595100511351383,\n",
       " 3.861798071828841,\n",
       " 3.204112783169879,\n",
       " 1.1344355941385231,\n",
       " 3.5102986122361246,\n",
       " 3.7754128924866697,\n",
       " 3.482166954647678,\n",
       " 3.6616942756027693,\n",
       " 2.9689110949365363,\n",
       " 3.849977358656628,\n",
       " 4.4901658192407945,\n",
       " 3.2624129012328873,\n",
       " 2.7120216339556995,\n",
       " 3.9752003866372707,\n",
       " 3.987318729299576,\n",
       " 4.298042073140507,\n",
       " 3.3299983458205653,\n",
       " 4.195093149815992,\n",
       " 3.0364485280849354,\n",
       " 3.46798112652078,\n",
       " 4.6731421276049305,\n",
       " 3.563789034544884,\n",
       " 3.2333403272909025,\n",
       " 3.2314610274508495,\n",
       " 3.2487635278970957,\n",
       " 2.7848907136319148,\n",
       " 4.179238791752969,\n",
       " 4.24220397295648,\n",
       " 3.8389999119775737,\n",
       " 3.907562229757119,\n",
       " 4.144930382657527,\n",
       " 3.342704766242913,\n",
       " 2.813150030665884,\n",
       " 1.600852249697402,\n",
       " 3.13973975062618,\n",
       " 3.083532318343674,\n",
       " 3.584996784694833,\n",
       " 2.9374581713557593,\n",
       " 3.6929515319219215,\n",
       " 2.464206798033767,\n",
       " 3.729948194660628,\n",
       " 3.4869510275503917,\n",
       " 3.7274039812054602,\n",
       " 4.530887123747958,\n",
       " 3.3610833616209357,\n",
       " 3.3735251551490792,\n",
       " 1.9302853845917816,\n",
       " 3.987920006485013,\n",
       " 2.8853124185976466,\n",
       " 3.1638877506898693,\n",
       " 2.722310819738833,\n",
       " 3.7910659696833964,\n",
       " 4.5595825806467545,\n",
       " 3.06661697230244,\n",
       " 3.6463892766604027,\n",
       " 3.2260378590037257,\n",
       " 4.537950456809608,\n",
       " 3.7583990424243843,\n",
       " 4.189907357942639,\n",
       " 3.4922256551575415,\n",
       " 3.9361361186862878,\n",
       " 4.349158120392502,\n",
       " 3.2234335529159415,\n",
       " 2.015083825763924,\n",
       " 3.783994304667957,\n",
       " 3.687498014267054,\n",
       " 2.51140054241493,\n",
       " 3.1733362149828843,\n",
       " 2.0570238157296195,\n",
       " 3.542356095211308,\n",
       " 4.221184176534967,\n",
       " 3.504209937137942,\n",
       " 2.9023592880937676,\n",
       " 3.997599603780054,\n",
       " 2.794826498494596,\n",
       " 4.028664838084719,\n",
       " 3.1280050161993924,\n",
       " 4.011585199979226,\n",
       " 4.215147762084741,\n",
       " 2.7640096910604863,\n",
       " 3.125492956229755,\n",
       " 4.533399902459109,\n",
       " 3.6416738141603813,\n",
       " 4.659753786134916,\n",
       " 4.01633494944612,\n",
       " 3.8120353524020008,\n",
       " 4.260672613866483,\n",
       " 3.887963780667673,\n",
       " 2.5766848897145875,\n",
       " 3.9465586937613217,\n",
       " 3.643013530399335,\n",
       " 3.636675189729584,\n",
       " 3.9207593107802614,\n",
       " 3.9173398484311317,\n",
       " 4.140640841176166,\n",
       " 4.667500303525243,\n",
       " 3.523630119993184,\n",
       " 3.3635789364422433,\n",
       " 3.9885763118826145,\n",
       " 3.4449413250145415,\n",
       " 4.130576019185507,\n",
       " 3.9257573248238558,\n",
       " 3.102990626125295,\n",
       " 2.7007591012758394,\n",
       " 3.1474838811212713,\n",
       " 3.7889427554021573,\n",
       " 3.9981427263395286,\n",
       " 4.504320987600207,\n",
       " 4.381351032725671,\n",
       " 4.204462145310867,\n",
       " 3.0703972059523372,\n",
       " 3.3165781971531616,\n",
       " 3.7027856315414973,\n",
       " 2.271739366046106,\n",
       " 4.4273488428860475,\n",
       " 3.4041131265617115,\n",
       " 4.197680619735769,\n",
       " 2.8939854368144275,\n",
       " 4.052060459883357,\n",
       " 3.7078833217865914,\n",
       " 4.218950940757703,\n",
       " 1.9835814880579408,\n",
       " 4.378152769959625,\n",
       " 3.4111933625598847,\n",
       " 1.7591715264464003,\n",
       " 3.8127848978607153,\n",
       " 3.3363825056992527,\n",
       " 2.661180124793876,\n",
       " 3.960248941261924,\n",
       " 3.5555448446640248,\n",
       " 3.8693032578191717,\n",
       " 2.8963604010907034,\n",
       " 3.82937042680205,\n",
       " 3.6360938763848907,\n",
       " 3.458745859423672,\n",
       " 3.43789444656652,\n",
       " 3.348375912158323,\n",
       " 3.773743788402072,\n",
       " 3.611848097280124,\n",
       " 3.8870350489234045,\n",
       " 3.057407167348379,\n",
       " 4.107615132961873,\n",
       " 3.510671663879115,\n",
       " 3.6213684817035428,\n",
       " 4.061523890506323,\n",
       " 3.410008579468938,\n",
       " 2.9085496032539733,\n",
       " 4.399829598466955,\n",
       " 2.577063202154617,\n",
       " 3.8417179959318934,\n",
       " 3.6894060551357195,\n",
       " 4.375783903407921,\n",
       " 4.281312330400684,\n",
       " 3.4846490917351467,\n",
       " 1.885652992638364,\n",
       " 3.718814712145008,\n",
       " 2.4938080287834135,\n",
       " 4.045289166347882,\n",
       " 3.495304769253285,\n",
       " 3.840477454944315,\n",
       " 4.279071298602407,\n",
       " 3.1689255957962086,\n",
       " 4.022821944853959,\n",
       " 2.313796273363209,\n",
       " 3.564161651934212,\n",
       " 4.101826393746688,\n",
       " 3.8503674003416304,\n",
       " 3.9307596212925566,\n",
       " 3.43541326782409,\n",
       " 2.827124692583425,\n",
       " 3.991573605846472,\n",
       " 3.1367268734769937,\n",
       " 4.231731363235764,\n",
       " 3.7022297603026857,\n",
       " 3.520884101484446,\n",
       " 3.18197485384154,\n",
       " 2.5377572146493113,\n",
       " 3.865243022039207,\n",
       " 2.5897690415453654,\n",
       " 3.771959352293959,\n",
       " 2.1647102644737366,\n",
       " 3.806718658922485,\n",
       " 4.388794413594184,\n",
       " 4.552225775815501,\n",
       " 3.506927450337271,\n",
       " 4.243908130151295,\n",
       " 3.1816363236566785,\n",
       " 3.1634708081358567,\n",
       " 3.4552720341436998,\n",
       " 4.400224739617082,\n",
       " 3.7858565856558615,\n",
       " 2.979123728705894,\n",
       " 3.5902649083164317,\n",
       " 4.460313111450176,\n",
       " 3.2120556861083354,\n",
       " 4.08190556364766,\n",
       " 4.622422613055948,\n",
       " 3.7502059750826415,\n",
       " 4.322213203640066,\n",
       " 2.096574994533442,\n",
       " 2.9672439575241842,\n",
       " 4.110985667303791,\n",
       " 3.583858050403505,\n",
       " 3.317051592092362,\n",
       " 4.0948411764739,\n",
       " 3.970911347887421,\n",
       " 4.42090527328317,\n",
       " 3.1447823195015783,\n",
       " 3.6479797957525038,\n",
       " 3.148294382156221,\n",
       " 4.253007541135287,\n",
       " 3.4469499056037454,\n",
       " 3.950709864022374,\n",
       " 3.862494588517388,\n",
       " 2.2360719992170734,\n",
       " 3.838629814473191,\n",
       " 3.9447308632042044,\n",
       " 4.101529629550573,\n",
       " 3.717393704150966,\n",
       " 3.331159020089172,\n",
       " 3.3078487598805926,\n",
       " 3.0892967727025478,\n",
       " 3.490845004726115,\n",
       " 4.2895452815369675,\n",
       " 3.057025433694246,\n",
       " 3.7533386436238296,\n",
       " 4.080521158948762,\n",
       " 2.982685761518989,\n",
       " 3.196739058540928,\n",
       " 4.820101705313586,\n",
       " 4.279885709195953,\n",
       " 1.45949723744711,\n",
       " 4.3600558458149425,\n",
       " 3.1881819247021936,\n",
       " 2.770416170382304,\n",
       " 2.557795843280954,\n",
       " 4.385607808292125,\n",
       " 4.41951516159864,\n",
       " 3.4997768863789283,\n",
       " 3.488200878042418,\n",
       " 4.04044867821041,\n",
       " 3.5043776388106522,\n",
       " 3.8108552878229904,\n",
       " 3.3357340052317057,\n",
       " 3.8130853592415885,\n",
       " 3.8586886476210402,\n",
       " 4.3730936550010195,\n",
       " 3.653304757398415,\n",
       " 4.42029522467235,\n",
       " 3.283628417297482,\n",
       " 5.168490244964439,\n",
       " 3.601794179453261,\n",
       " 1.582535774890758,\n",
       " 4.151014207835612,\n",
       " 3.791596607520388,\n",
       " 3.420316791358132,\n",
       " 3.983713000879808,\n",
       " 3.93449499091409,\n",
       " 3.756838965244955,\n",
       " 3.623348452088938,\n",
       " 3.5657939147282223,\n",
       " 3.3185287151572025,\n",
       " 3.288516486831297,\n",
       " 3.5569197291714603,\n",
       " 3.689290707749181,\n",
       " 3.6009915903460676,\n",
       " 3.6207494544199608,\n",
       " 3.2661897762421597,\n",
       " 3.6405860268364836,\n",
       " 2.8509751321255346,\n",
       " 3.836430437737573,\n",
       " 2.952339324363626,\n",
       " 3.641863846230988,\n",
       " 2.548890088368144,\n",
       " 3.9060476392183365,\n",
       " 4.376537767526031,\n",
       " 3.3180228351386534,\n",
       " 2.8715265416965514,\n",
       " 3.970105436504482,\n",
       " 3.894094258429043,\n",
       " 3.4229653068247985,\n",
       " 4.19756671868271,\n",
       " 3.117920260341371,\n",
       " 4.676447387304621,\n",
       " 3.183280525737904,\n",
       " 1.0566683118841742,\n",
       " 3.4661798823224665,\n",
       " 3.6567428524768006,\n",
       " 3.9599757619470246,\n",
       " 3.8490802526088017,\n",
       " 3.8016376378133074,\n",
       " 3.5461502480671365,\n",
       " 4.085148865443592,\n",
       " 3.5721377440272963,\n",
       " 3.214588700257611,\n",
       " 3.9125113870476556,\n",
       " 4.18437229238096,\n",
       " 3.717644912166187,\n",
       " 2.923347182860401,\n",
       " 4.320463746945435,\n",
       " 3.0417191691971612,\n",
       " 3.4225638247213825,\n",
       " 2.5278104786877185,\n",
       " 3.7119351216249816,\n",
       " 2.7844397480688237,\n",
       " 3.4293676601090786,\n",
       " 3.7023427738661123,\n",
       " 1.9876602537475774,\n",
       " 3.7479725095763237,\n",
       " 3.5405208936576202,\n",
       " 3.664361656974055,\n",
       " 3.274583113742106,\n",
       " 4.568688964909792,\n",
       " 3.904253130177429,\n",
       " 3.6859497848738814,\n",
       " 4.4241018077491265,\n",
       " 2.381673474339298,\n",
       " 3.487309711921736,\n",
       " 4.264664194271989,\n",
       " 4.162320323806461,\n",
       " 2.523979490136362,\n",
       " 4.219087566499336,\n",
       " 4.329698090060691,\n",
       " 2.849994943299967,\n",
       " 3.3129036417773037,\n",
       " 3.7283777832907377,\n",
       " 2.3301060806248453,\n",
       " 3.3002591782960913,\n",
       " 2.870994300495778,\n",
       " 4.163602214139068,\n",
       " 4.286294161350794,\n",
       " 3.0171782228136634,\n",
       " 2.9653916190563283,\n",
       " 4.757813177496196,\n",
       " 4.411439732164908,\n",
       " 3.869350148576376,\n",
       " 2.1828407174742472,\n",
       " 4.017494506079119,\n",
       " 3.349666206342957,\n",
       " 2.885964627631432,\n",
       " 4.243506312105897,\n",
       " 3.20972624360793,\n",
       " 4.519313645466792,\n",
       " 2.6730329499775465,\n",
       " 3.6925602240147977,\n",
       " 2.583370765380221,\n",
       " 3.5343639153067135,\n",
       " 4.067780784862625,\n",
       " 3.985044616176438,\n",
       " 3.8003883572786976,\n",
       " 4.064530232102216,\n",
       " 2.7261376951704523,\n",
       " 3.024139593562454,\n",
       " 2.040756780864962,\n",
       " 2.2850149501029926,\n",
       " 3.99572212945368,\n",
       " 4.217101514102087,\n",
       " 4.6893371068752865,\n",
       " 4.079036419399524,\n",
       " 3.6058221128796,\n",
       " 3.379253987673002,\n",
       " 3.018449583854446,\n",
       " 3.0746617795867808,\n",
       " 2.5621234138704927,\n",
       " 2.73629356067993,\n",
       " 3.2173318777873483,\n",
       " 3.7131783333480026,\n",
       " 3.7325013235956783,\n",
       " 3.315659512931231,\n",
       " 5.193722473714879,\n",
       " 4.355182207531647,\n",
       " 4.0606839111636255,\n",
       " 3.974119220199451,\n",
       " 3.4412841243351977,\n",
       " 4.209024223263397,\n",
       " 3.9941952111491004,\n",
       " 2.1987178618898677,\n",
       " 3.7902542611376346,\n",
       " 4.075626937958593,\n",
       " 2.744758472658038,\n",
       " 3.2470319447987097,\n",
       " 5.069168354466443,\n",
       " 3.529801579032736,\n",
       " 3.5762526821838536,\n",
       " 3.775157883959246,\n",
       " 2.9171732280133753,\n",
       " 3.525912924739481,\n",
       " 3.1029136337150725,\n",
       " 3.911767989404715,\n",
       " 2.656602423921825,\n",
       " 3.4339630212516177,\n",
       " 2.9489387432002534,\n",
       " 4.5508681883918145,\n",
       " 2.365678257006495,\n",
       " 4.011030756789254,\n",
       " 3.6687716729441764,\n",
       " 2.998889176422742,\n",
       " 4.2831443291806295,\n",
       " 3.336794377942723,\n",
       " 2.9382525493128058,\n",
       " 3.591670782559269,\n",
       " 3.790474143365012,\n",
       " 4.149945933539753,\n",
       " 2.2333618741978833,\n",
       " 4.163086963894291,\n",
       " 4.031783992671154,\n",
       " 4.288204351516285,\n",
       " 4.241674555263057,\n",
       " 4.036239435824241,\n",
       " 3.446840219558726,\n",
       " 4.415203616614153,\n",
       " 3.3007358248612477,\n",
       " 1.042869145877241,\n",
       " 3.8401112380709623,\n",
       " 3.642017468940879,\n",
       " 2.9043116330345944,\n",
       " 3.2957874378598118,\n",
       " 4.07363379769005,\n",
       " 3.942572612422464,\n",
       " 2.9488114678939064,\n",
       " 3.276538038247721,\n",
       " 3.3779034977538536,\n",
       " 2.8268153782691585,\n",
       " 4.262416874207866,\n",
       " 2.842873623461451,\n",
       " 3.3707815306446496,\n",
       " 4.1114722668344035,\n",
       " 4.4395756723814985,\n",
       " 3.115830926996335,\n",
       " 3.854259486895164,\n",
       " 3.6795711422521715,\n",
       " 4.40384272246958,\n",
       " 4.4008694884350135,\n",
       " 4.396539422189381,\n",
       " 3.0714498724409087,\n",
       " 3.306362062864819,\n",
       " 3.4298701349067975,\n",
       " 4.116918077117763,\n",
       " 3.6626428447209096,\n",
       " 4.525143919420696,\n",
       " 3.5465815855046907,\n",
       " 3.206004724468832,\n",
       " 4.080896873636047,\n",
       " 4.686651718847663,\n",
       " 3.8921386682830867,\n",
       " 4.873740354742704,\n",
       " 3.9826746100470367,\n",
       " 2.648789053081215,\n",
       " 4.02705531845745,\n",
       " 3.1603059002499605,\n",
       " 2.7651906535876796,\n",
       " 3.6510500632588516,\n",
       " 3.2285055395563402,\n",
       " 3.2662061858229148,\n",
       " 4.5674258607609435,\n",
       " 3.8871275125936755,\n",
       " 3.5510419006765557,\n",
       " 3.964433388070719,\n",
       " 3.563088588260968,\n",
       " 2.60425160783514,\n",
       " 4.62929370236447,\n",
       " 3.949036419172848,\n",
       " 3.8345500987408614,\n",
       " 3.962236634706829,\n",
       " 3.8900316283982246,\n",
       " 3.562520690043942,\n",
       " 3.6719583449520736,\n",
       " 3.344858410704347,\n",
       " 2.632934296863109,\n",
       " 3.1062895576604608,\n",
       " 2.787690033704005,\n",
       " 3.7494029846773813,\n",
       " 3.4890427398078856,\n",
       " 3.81538056865505,\n",
       " 2.660164474097402,\n",
       " 4.21091509668956,\n",
       " 2.0944207443721243,\n",
       " 3.8199298207807457,\n",
       " 2.720299244103214,\n",
       " 4.08147672232269,\n",
       " 3.91658444866344,\n",
       " 4.279170785558119,\n",
       " 3.281742669495236,\n",
       " 4.101470594454416,\n",
       " 4.244396030390238,\n",
       " 3.244595963422271,\n",
       " 2.6365435329149363,\n",
       " 4.08103985836128,\n",
       " 3.3310342564518653,\n",
       " 3.053163719154937,\n",
       " 3.0279813095983803,\n",
       " 3.4637930574010554,\n",
       " 4.026797102900281,\n",
       " 3.7334694504058157,\n",
       " 3.33507077199992,\n",
       " 3.5979377285168495,\n",
       " 2.3685247477296,\n",
       " 3.7331043927957785,\n",
       " 4.610785952134522,\n",
       " 3.757712908099932,\n",
       " 4.401865074770455,\n",
       " 2.0429366248589607,\n",
       " 2.155471207459773,\n",
       " 4.039544460797435,\n",
       " 3.2436195287824914,\n",
       " 3.9029679275274063,\n",
       " 3.3767750178610734,\n",
       " 4.222303309896993,\n",
       " 2.9032737530195605,\n",
       " 3.820965951661782,\n",
       " 2.121072258435037,\n",
       " 3.240565868901663,\n",
       " 3.203007115669739,\n",
       " 3.505086661760039,\n",
       " 4.270484423979896,\n",
       " 3.0948576719878607,\n",
       " 3.353916023794428,\n",
       " 4.3099707423131335,\n",
       " 2.088339765709641,\n",
       " 2.5998145107901567,\n",
       " 4.260190505164588,\n",
       " 3.168847859052675,\n",
       " 3.7715412703289846,\n",
       " 2.7780682939213763,\n",
       " 4.201726484583076,\n",
       " 2.963957464245369,\n",
       " 3.936119409365638,\n",
       " 3.5215544777268173,\n",
       " 4.252703090665868,\n",
       " 3.351047203598518,\n",
       " 4.61480254053398,\n",
       " 4.55055787755205,\n",
       " 3.8923333114197733,\n",
       " 4.257988832068243,\n",
       " 3.5308264937584677,\n",
       " 3.160869467262624,\n",
       " 3.7453615110489076,\n",
       " 3.0205305802217626,\n",
       " 2.9762934637327465,\n",
       " 3.691126021813927,\n",
       " 3.775159800065889,\n",
       " 3.5113075583708455,\n",
       " 2.6924018211653924,\n",
       " 3.9031705498552314,\n",
       " 3.547272888731961,\n",
       " 4.583188459014821,\n",
       " 3.0177872814057296,\n",
       " 3.5922668087534007,\n",
       " 2.793304694392755,\n",
       " 3.4942800364338904,\n",
       " 3.939573442796552,\n",
       " 2.8954310156499474,\n",
       " 3.453661006181759,\n",
       " 3.043505345162261,\n",
       " 3.1434840154905554,\n",
       " 4.056350051198287,\n",
       " 2.7317396749235554,\n",
       " 2.32965985537026,\n",
       " 4.336230727287981,\n",
       " 4.032118440436688,\n",
       " 1.7744754861475818,\n",
       " 4.258922582965855,\n",
       " 3.352303919439964,\n",
       " 3.5704791381453056,\n",
       " 1.9401873799666545,\n",
       " 2.356760356133746,\n",
       " 2.400160502898205,\n",
       " 4.39796628124267,\n",
       " 3.4621990498599047,\n",
       " 3.183037588205951,\n",
       " 4.074158798124624,\n",
       " 2.961746562266117,\n",
       " 2.7194871645924006,\n",
       " 3.121927121166893,\n",
       " 2.4694130883980017,\n",
       " 4.741970612167002,\n",
       " 3.2771309363603516,\n",
       " 3.378565723406389,\n",
       " 4.134396848644504,\n",
       " 3.708437477884217,\n",
       " 3.2867878191944624,\n",
       " 2.5072622762972268,\n",
       " 3.0108547208265253,\n",
       " 2.6942287774398768,\n",
       " 4.144027775621271,\n",
       " 3.507634227240646,\n",
       " 3.3204384436850867,\n",
       " 2.295572195985729,\n",
       " 2.9904759562887553,\n",
       " 3.5946456390162433,\n",
       " 3.9612831487418263,\n",
       " 3.8802921049869212,\n",
       " 3.3706548611671407,\n",
       " 2.9866265961468406,\n",
       " 2.9429468546247883,\n",
       " 3.2846181226857363,\n",
       " 4.048892092124704,\n",
       " 3.563894134987806,\n",
       " 2.7013221570516017,\n",
       " 3.8259215199753047,\n",
       " 3.0215279545833416,\n",
       " 3.8322070981257483,\n",
       " 3.2374051501360968,\n",
       " 2.7905153303429318,\n",
       " 2.9316993542168075,\n",
       " 3.098436929452397,\n",
       " 3.2534589526573963,\n",
       " 3.7670079391463354,\n",
       " 4.102452549298876,\n",
       " 3.4896410344183257,\n",
       " 3.238410574961998,\n",
       " 3.0398253028669715,\n",
       " 3.922091824776528,\n",
       " 3.537934574798592,\n",
       " 4.025834221400436,\n",
       " 3.0500952131166783,\n",
       " 2.657952166819147,\n",
       " 4.139572104200678,\n",
       " 3.96084817425883,\n",
       " 4.393885473180119,\n",
       " 3.805718304629163,\n",
       " 1.9462107999238982,\n",
       " 2.579845328644094,\n",
       " 2.8518902111421993,\n",
       " 3.40404463720998,\n",
       " 2.3565614220892472,\n",
       " 4.064076293691528,\n",
       " 3.9318235603085,\n",
       " 3.2087598778242334,\n",
       " 3.6289669392992545,\n",
       " 3.205492401759858,\n",
       " 3.5559018193780263,\n",
       " 3.1133490277823923,\n",
       " 2.0754275305849545,\n",
       " 3.0941508926950356,\n",
       " 2.8068521704582596,\n",
       " 4.257512179274301,\n",
       " 2.8726779977025325,\n",
       " 3.6658319918441093,\n",
       " 3.693099171415074,\n",
       " 3.596471738010555,\n",
       " 4.031170956563563,\n",
       " 3.889738248566255,\n",
       " 4.215857181459092,\n",
       " 3.9073573961895076,\n",
       " 3.2678046467906134,\n",
       " 4.043255301796063,\n",
       " 3.4805382958189357,\n",
       " 3.4674486255093804,\n",
       " 3.5731919088834925,\n",
       " 3.8023124940935458,\n",
       " 3.6199628276895237,\n",
       " 3.83438352044234,\n",
       " 4.133020000539357,\n",
       " 3.8835320423465642,\n",
       " 4.0568899377100625,\n",
       " 3.9482440583187333,\n",
       " 4.22337512776149,\n",
       " 3.063311735839968,\n",
       " 3.9955931924153223,\n",
       " 3.3464988581177133,\n",
       " 3.229113486828749,\n",
       " 2.7079515299797583,\n",
       " 2.799324698597464,\n",
       " 4.523335098261188,\n",
       " 3.5747274971224727,\n",
       " 2.7590857926786,\n",
       " 2.912272472082029,\n",
       " 4.264037204615908,\n",
       " 3.6540760112947783,\n",
       " 3.4046544401360204,\n",
       " 3.1132668444770673,\n",
       " 3.3307996658895394,\n",
       " 4.193469818651266,\n",
       " 3.408273028994471,\n",
       " 3.5144375258430984,\n",
       " 4.0328570619863395,\n",
       " 2.6547109120498726,\n",
       " 3.57469817264425,\n",
       " 2.159243077188308,\n",
       " 3.4353709413948996,\n",
       " 4.202850601731936,\n",
       " 3.7816258207666396,\n",
       " 4.165229566824421,\n",
       " 3.398096847612441,\n",
       " 3.7115406407180886,\n",
       " 3.4998043528220713,\n",
       " 3.2395371615677506,\n",
       " 2.9840188788566064,\n",
       " 2.6334547051044863,\n",
       " 3.5180931628161147,\n",
       " 3.9892373227336084,\n",
       " 3.6333874177758343,\n",
       " 3.096303728452581,\n",
       " 4.194623860971665,\n",
       " 3.7327240897321046,\n",
       " 3.9649924524749838,\n",
       " 3.7536178581540787,\n",
       " 2.538338109199261,\n",
       " 3.108797312160311,\n",
       " 3.5302342832054427,\n",
       " 2.9030518898851345,\n",
       " 3.037319300068447,\n",
       " 3.0345469155818776,\n",
       " 4.29302226461021,\n",
       " 3.459447372841975,\n",
       " 4.106652956015643,\n",
       " 4.072954791968892,\n",
       " 2.7562985538460776,\n",
       " 3.7294716075334526,\n",
       " 4.545132339413939,\n",
       " 4.293595902751353,\n",
       " 3.753362917775329,\n",
       " 2.9451563388329984,\n",
       " 3.4305650223258013,\n",
       " 3.8542191644282204,\n",
       " 3.782094126283979,\n",
       " 2.265301259095987,\n",
       " 3.296076724421289,\n",
       " 4.550955509980563,\n",
       " 2.4908896741671507,\n",
       " 3.938093053535155,\n",
       " 4.349846586885575,\n",
       " 3.422498257079245,\n",
       " 3.1749922503002876,\n",
       " 4.403634405031674,\n",
       " 3.8782954343462572,\n",
       " 3.307507194899645,\n",
       " 3.923428588248519,\n",
       " 3.7925989872743053,\n",
       " 3.01972622306191,\n",
       " 2.44334222074923,\n",
       " 3.745557309355132,\n",
       " 4.077438833290935,\n",
       " 2.7464883595157463,\n",
       " 3.1800606198358836,\n",
       " 3.723800580751772,\n",
       " 2.7696068772193096,\n",
       " 2.110875840724466,\n",
       " 3.9188101565773423,\n",
       " 3.066238057412408,\n",
       " 2.3947035994819252,\n",
       " 2.8901434785311433,\n",
       " 3.73957402280925,\n",
       " 4.361999110707083,\n",
       " 2.425962990650004,\n",
       " 3.4138449752791735,\n",
       " 2.194017101311411,\n",
       " 4.232491825059214,\n",
       " 1.9047905522619741,\n",
       " 3.7661602233728284,\n",
       " 3.344619951306748,\n",
       " 2.459339849879467,\n",
       " 3.896364563400826,\n",
       " 3.243952035380171,\n",
       " 2.908128527867466,\n",
       " 3.4499214732170436,\n",
       " 2.27172646889131,\n",
       " 4.222932602089156,\n",
       " 4.43298596696989,\n",
       " 3.6210713643706343,\n",
       " 3.4483224081430013,\n",
       " 3.7434351150375718,\n",
       " 3.9851472689077445,\n",
       " 3.5875992460338018,\n",
       " 3.6569627942779226,\n",
       " 3.523553778041642,\n",
       " 3.483105656199979,\n",
       " 4.079764839885448,\n",
       " 3.35433283747458,\n",
       " 3.528030047262947,\n",
       " 3.0332943881289816,\n",
       " 3.29305113526557,\n",
       " 2.6767341115681877,\n",
       " 3.0364364167991416,\n",
       " 3.910237927164245,\n",
       " 3.9563728640966214,\n",
       " 3.361313453739097,\n",
       " 3.4520330952483187,\n",
       " 3.363002273028263,\n",
       " 3.6763996917951483,\n",
       " 3.7068127883571904,\n",
       " 2.563487125811271,\n",
       " 3.1649825982438786,\n",
       " 3.1573235464284872,\n",
       " 3.3830345431587285,\n",
       " 2.5906582059760606,\n",
       " 3.7547888906766897,\n",
       " 4.277884363827239,\n",
       " ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kiKj-M6d2a-Y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89992, 89992, 50)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(users_details), len(movies_details)\n",
    "len(u_i),len(v_j),len(all_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NeWAGkT6C9kq"
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lkIQOOo1C9o7"
   },
   "source": [
    "<font color='red'> Task 2</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know U is the learned matrix of user vectors, with its i-th row as the vector ui for user i. Each row of U can be seen as a \"feature vector\" for a particular user.\n",
    "\n",
    "The question we'd like to investigate is this: do our computed per-user features that are optimized for predicting movie ratings contain anything to do with gender?\n",
    "\n",
    "The provided data file <a href='https://drive.google.com/open?id=1PHFdJh_4gIPiLH5Q4UErH8GK71hTrzlY'>user_info.csv</a> contains an is_male column indicating which users in the dataset are male. Can you predict this signal given the features U?\n",
    "\n",
    "\n",
    "> __Note 1__ : there is no train test split in the data, the goal of this assignment is to give an intution about how to do matrix factorization with the help of SGD and application of truncated SVD. for better understanding of the collabarative fillerting please check netflix case study. <br><br>\n",
    "> __Note 2__ : Check if scaling of $U$, $V$ matrices improve the metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9kl4Ryi_7E_T"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1FTc39gDdti"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 943)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 943 entries, 0 to 942\n",
      "Data columns (total 4 columns):\n",
      "user_id         943 non-null int64\n",
      "age             943 non-null int64\n",
      "is_male         943 non-null int64\n",
      "orig_user_id    943 non-null int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 29.6 KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('user_info.csv.txt')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_age = np.array(data['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding the new axis of age to users\n",
    "age_newaxis = np_age[:,np.newaxis]\n",
    "age_newaxis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_info_new_data = np.hstack((U, age_newaxis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 944)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_info_new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#appyling logestic regression on the data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "User_models = LogisticRegression(random_state=0).fit(user_info_new_data, data['is_male'], )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing model predictions\n",
    "model_predictions = User_models.predict(user_info_new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7104984093319194"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check f1 scrore metris\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "f1_score(list(data['is_male']),list(model_predictions),average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7104984093319194"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the accuracy\n",
    "accuracy_score(list(data['is_male']),list(model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "''' \n",
    "     1. Even data is important *** \n",
    "     2. This assignment gave intution of adjacent matrix - SVD \n",
    "     3. After adding other features how it behavous show in task-2 \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Recommendation_system_assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
